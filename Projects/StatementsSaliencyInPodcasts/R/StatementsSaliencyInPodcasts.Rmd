---
title: "Statements saliency in podcasts"
author: "Anton Antonov"
date: "September 18, 2016"
output: html_document
---
<!--- title: "Summarization of Freakonomics podcasts" -->
<!--- 
rmarkdown::render( input = "./StatementsSaliencyInPodcasts.Rmd", output_file = "./StatementsSaliencyInPodcasts.html")
rmarkdown::render( input = "./StatementsSaliencyInPodcasts.Rmd", output_file = "./StatementsSaliencyInPodcasts.pdf")

--->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries (and code) needed:

```{r} 
library(plyr)
library(httr)
library(XML)
library(rvest)
library(irlba)
library(devtools)
library(lattice)

if ( !exists("SMRMakeDocumentTermMatrix") ) {
  source_url( "https://raw.githubusercontent.com/antononcube/MathematicaForPrediction/master/R/DocumentTermWeightFunctions.R" )
}
```


## Introduction

This document is produced from an executable R-Markdown file available at the GitHub repository [MathematicaVsR](https://github.com/antononcube/MathematicaVsR).

The goal for this project is two-fold:

1. to experiment (in R) with algebraic computations determination of the most important sentences in natural language texts, and

2. to compare the programming code of the developed functionalities with a similar project in *Mathematica*.

In order to make those experiments we have to find, choose, and download suitable text data. This document gives a walk through with code of the complete sequence of steps from intent to experimental results. 

The following concrete steps are taken.

1. Data selection of a source that provides high quality texts. (E.g. English grammar, spelling, etc.)

2. Download or scraping of the text data.

3. Text data parsing, cleaning, and other pre-processing.

4. Mapping of a selected document into linear vector space using the Bag-of-words model.

5. Finding sentence/statement saliency using matrix algebra.

6. Experimenting with the saliency algorithm over the data and making a suitable interactive interface. 


## Data source selection (Freakonomics podcasts transcripts)

We want to select a text data source from which we can easily obtain documents, the documents are easier pre-process, and -- most importantly -- with interesting content since we want to validate our sumarization techniques and for that some fraction of the documents has to be read in full.

One such source is the archive of Freakonomics podcast transcripts available at [http://freakonomics.com/](http://freakonomics.com/).

( After the publication of the book "Freakonomics" its authors Levitt and Dubner wrote another books "SuperFreakonomics" and started a radio show. See [http://freakonomics.com/about/](http://freakonomics.com/about/) for more details. )

## Getting the data 

First we are going to find the links to the podcast transcripts and then download each transcript.
For both sub-tasks we are going to use the package [rvest](https://github.com/hadley/rvest) written by [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham).

### Podcast transcripts links

The code below goes through set of pages and extracts the hyperlinks that finish with the string "full-transcript/".

```{r}
if ( !exists("podcastLinks") ) {
  podcastLinks <- 
    llply( 1:17, function(pg) { 
      url <- paste( "http://www.freakonomics.com/category/podcast-transcripts/page/", pg, sep="")
      x <- GET(url, add_headers('user-agent' = 'r'))
      res <- read_html( x = x )
      hlinks <- html_attr( html_nodes( res, "a"), "href")
      grep( pattern = "full-transcript/$", x = hlinks, value = TRUE )
    }, .progress = "none" ) 
  
  podcastLinks <- unlist(podcastLinks)
}
```

### Get podcast transcripts

The following code goes through each podcast links downloads its content.

```{r}
if ( !exists("podcastPages") ) {
  cat( "computed 'podcastPages' again ... \n")
  podcastPages <-
    llply( podcastLinks, function(hl) {
      x <- GET( hl, add_headers('user-agent' = 'r'))
      read_html( x = x )
    }, .progress = "none" )
}
```

The following code picks paragraphs from the content of each page.

```{r}
podcastTexts <- 
  llply( podcastPages, function(pg) {
    html_text( html_nodes( pg, "p" ) )
  }, .progress = "none" )
```

Each of the obtained podcast transcripts are represented as list of stings, each string corresponding to a paragraph in the transcript HTML file.

The following code takes the title of each page.

```{r}
podcastTitles <- 
  llply( podcastPages, function(pg) {
    html_text( html_nodes( pg, "title" ))
  }, .progress = "none" )
```

The following code removes redundant parts of the titles.

```{r}
pat <- "Full Transcript - Freakonomics Freakonomics"
podcastTitles <- gsub( pattern = pat, replacement = "", x = podcastTitles, fixed = TRUE )
podcastTitles <- gsub( pattern = paste( ":", pat ), replacement = "", x = podcastTitles, fixed = TRUE )
```

Here is how the titles look like:

```{r}
sample(podcastTitles,6)
```

## Transcripts processing

### Removing of non-statements

The pattern `"^\\[[[:upper:]]*"` is used to remove lines like: `"[MUSIC: Interkosmos, “Tickticktock” (from London Mix)]"`.

```{r}
podcastTexts2 <- 
  llply( podcastTexts, function(plines) {
    grep( pattern = "^\\[[[:upper:]]*", x = plines, invert = TRUE, value = TRUE )
  }, .progress = "none" )
```

For example, see the output of this command :
```{r}
grep( pattern = "^\\[[[:upper:]]*", x = c("[MUSIC: Interkosmos, “Tickticktock” (from London Mix)]"), value = TRUE)
```

### Seleting paragraphs that start with human names

The next transformation is to select paragraphs that start with strings like "DUBNER:" or "Sarah BOLT:".
```{r}
podcastTexts3 <- 
  llply( podcastTexts2, function(plines) {
    grep( pattern = "(^[[:upper:]]*\\:)|(^[[:alpha:]]\\W[[:upper:]]*\\:)", x = plines, value = TRUE )
  }, .progress = "none" )
```


### Optional dropping of the speker names

```{r}
if( TRUE) {
  podcastTexts4 <- 
    llply( podcastTexts3, function(plines) {
      res <- gsub( pattern = "^[[:upper:]]*\\:", replacement = "", x = plines )
      gsub( "^(\\W*)", "", res )
    }, .progress = "none" )
}
```

### Final result

After finishing the experiments with parsing we assign to the original variable.
```{r}
podcastTexts <- podcastTexts4
```

## The Bag-of-words model

One on the most established approaches in information retrieval is to use the so called [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model), [3]. With that model every words is an axis in a linear vector space, and a document is represented as a vector in that space.

We are going to use the function `DocumentTermMatrix` from the source file [DocumentTermWeightFunctions.R]() in order to convert a list of "documents" (strings) to a sparse matrix the rows of which correspond to the documents and the columns correspond to words (terms). 

Let us make a document-term matrix for all transcripts. Each entry $(i,j)$ of the obtained matrix is the count of how many times the word $j$ appeared in the document $i$. We are not going to use that matrix for the saliency computations, but we can compute some useful statistics with it. Note that with command below we first concatenated the paragraphs of each transcript.

```{r}
twMat <- SMRMakeDocumentTermMatrix( laply( podcastTexts, function(x) paste( x, collapse = " ") ), split = "\\W", applyWordStemming = FALSE, minWordLength = 1 )
```

The obtained matrix has the following dimensions: ```r dim(twMat)```.

Let us convert the obtained matrix into a binary matrix by replacing the non-zero elements with $1$. That matrix tells is the term $j$ found in the transcript $i$. 

```{r}
twMat01 <- twMat; twMat01@x[ twMat01@x > 0 ] <- 1
```

Next we can compute the distributions of the words across the transcripts and the transcripts across the words by summing respectively the columns and rows of the binary matrix.

```{r}
print( histogram( log(colSums(twMat),10), type = "count", main = "Transcripts per word distribution", xlab = "lg(transcripts)", ylab = "words" ), split = c(1,1,1,2), more = TRUE )
print( histogram( rowSums(twMat), type = "count", main = "Words per transcript distribution", xlab = "words", ylab = "transcripts" ), split = c(1,2,1,2), more = FALSE )

```


## Stop words

In order to get better result we have to remove the [stop words](https://en.wikipedia.org/wiki/Stop_words) [4] from the texts.
The following command downloads is a list stop words referenced in the Wikipedia entry "Stop words."

```{r}
if( !exists("stopWords") ) {
  stopWords <- read.table("http://www.textfixer.com/resources/common-english-words.txt", stringsAsFactors = FALSE)[[1]]
  stopWords <- strsplit( stopWords, "," )[[1]]
}
```

The number of stop words obtained is ```r length(stopWords)```. Here is a sample them:

```{r}
sample(stopWords,20)
```

Using the transcript term matrix we can find additional stop words that are specific for the transcript collection. We can ask which words have frequencies comparable with the stop words. These commands find the top most frequent words in the collection:

```{r}
fr<-colSums(twMat)[stopWords]; fr[order(-fr)[1:12]]/sum(twMat)
```

Compare with the list in the Wikipedia entry ["Most common words in English"](https://en.wikipedia.org/wiki/Most_common_words_in_English).

## Finding the most important sentences

At this point we are ready to program a function that is going calculate the most important statements in a podcast using the algebraic approach explained in [5] by Elden.

Here is the function definition:

```{r}
MostImportantSentences <- function( sentences, 
                                    nSentences = 5, 
                                    globalTermWeightFunction = "IDF", 
                                    split = "\\W", 
                                    applyWordStemming = FALSE,
                                    minWordLength = 2, 
                                    stopWords = NULL ) {
  
  ## Create a document-term matrix
  swMat <- SMRMakeDocumentTermMatrix( documents = sentences, split = split, applyWordStemming = applyWordStemming, minWordLength = minWordLength )
  
  ## Remove stop words. (Note that this done through the columns of the document-term matrix.)
  if ( !is.null(stopWords) ) {
    stopWords <- intersect( stopWords, colnames(swMat) )
    if ( length(stopWords) > 0 ) {
      swMat[, stopWords ] <- 0  
    }
  }
  
  ## Apply LSI weight functions.
  wswMat <- SMRApplyTermWeightFunctions( docTermMat = swMat, 
                                         globalWeightFunction = globalTermWeightFunction, localWeightFunction = "None", normalizerFunction = "Cosine" )
  
  ## Using Eigenvector decomposition
  # wstSMat <- wswMat %*% t(wswMat)
  # eres <- eigen( wstSMat )
  # svec <- eres$vectors[,1]
  
  ## Using SVD for most salient statements.
  svdRes <- irlba( A = wswMat, nv = nSentences )
  svec <- svdRes$u[,1]
  inds <- rev(order(abs(svec)))[1:nSentences]

  ## Final result
  data.frame( Score = abs(svec)[inds], Sentence = sentences[inds], stringsAsFactors = FALSE)
}
```

### Tests

```{r}
podcastTitles[[96]]
MostImportantSentences( sentences = podcastTexts[[96]], nSentences = 3, stopWords = stopWords, applyWordStemming = FALSE )
```

```{r}
podcastTitles[[124]]
MostImportantSentences( sentences = podcastTexts[[124]], nSentences = 3, stopWords = stopWords, applyWordStemming = FALSE )
```


### Interactive interface

Running the file ["StatementsSaliencyInPodcastsInterface.R"](https://github.com/antononcube/MathematicaVsR/blob/master/Projects/StatementsSaliencyInPodcasts/R/StatementsSaliencyInPodcastsInterface.R) would produce a Shiny interactive interface that allows to see effects of different parameter combinations.

[![Snapshot](http://imgur.com/7kGplsfl.png)](http://imgur.com/7kGplsf.png)

## References

[1] Anton Antonov, [MathematicaVsR project at GitHub](https://github.com/antononcube/MathematicaVsR).

[2] Anton Antonov, ["Implementation of document-term matrix re-weighting functions in R"](https://github.com/antononcube/MathematicaForPrediction/blob/master/R/DocumentTermWeightFunctions.R) (2014) at [MathematicaForPrediction project at GitHub](https://github.com/antononcube/MathematicaForPrediction).

[3] Wikipedia entry, [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model).

[4] Wikipedia entry, [Stop words](https://en.wikipedia.org/wiki/Stop_words).

[5] Lars Elden, Matrix Methods in Data Mining and Pattern Recognition, 2007, SIAM. 
See Chapter 13, "Automatic Key Word and Key Sentence Extraction". 


